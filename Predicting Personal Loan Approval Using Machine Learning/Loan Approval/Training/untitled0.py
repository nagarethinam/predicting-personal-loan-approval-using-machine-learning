# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mJ2Lt5CzIT5wgs1MDMO844JrzXfaJ6qU
"""

# Commented out IPython magic to ensure Python compatibility.
 import pandas as pd
 import numpy as np
 import pickle
 import matplotlib.pyplot as plt
#  %matplotlib inline
 import seaborn as sns
 import sklearn
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.model_selection import RandomizedSearchCV
 import imblearn
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler
 from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score

data = pd.read_csv('loan_prediction.csv')
data

data.info()

data.isnull().sum()

print(y.value_counts())
print(y_bal.value_counts())

from imblearn.combine import SMOTETomek

data.describe()

plt.figure(figsize=(12,5))
plt.subplot(121)
sns.distplot(data['ApplicantIncome'], color='r')
plt.subplot(122)
sns.distplot(data['Credit_History'])
plt.show()

#plotting the count plot
plt.figure(figsize=(18,4))
plt.subplot(1,4,1)
sns.countplot(data['Gender'])
plt.subplot(1,4,2)
sns.countplot(data['Education'])
plt.show()

plt.figure(figsize=(20,5))
plt.subplot(131)
sns.countplot(data['married'], hue=data['gender'])
plt.subplot(132)
sns.countplot(data['self_employed'],hue=data['education'])
plt.subplot(133)
sns.countplot(data['property_Area'],hue=data['Lone_Amount_term'])

sns.swarmplot(data['Gender'],data['ApplicantIncome'], hue = data['loan_status'])

y = data['Loan_Status']
x = data.drop(columns=['Loan_Status'],axis=1)

model_history = classifier.fit(x_train, y_train, batch_size=100,validation_split=0.2, epochs=100)

x_train, x_test, y_train, y_test= train_test_split(x_bal, y_bal, test_size=0.33, random_state=42)

def decisionTree(x_train, x_test, y_train, y_test)
    dt=DecisionTreeClassifier()
    dt.fit(x_train,y_train)
    ypred = dt.predict(x_test)
    print('***DecisionTreeClassifier***')
    print('Confusion matrix') 
    print(confusion_matrix(y_test,ypred))
    print('Classification report')
    print(classification_report(y_test,ypred))

def RandomForestClassifiest(x_train, x_test, y_train, y_test):
  rf =RandomForestClassifier()
  rf .fit(x_train,y_train)
  ypred= rf.predict(x_test)
  print('***RandomForestClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test,ypred))
  print('Clssification report')
  print(classification_report(y_test,ypred))

#printing the values of y before balancing the data and after
print(y.value_counts())
print(y_bal.value_counts())

def KNN(x_train, x_test, y_train, y_test):
    knn = KNeighborsClassifier()
    knn.fit(x_train,y_train)
    yPred = knn.predict(x_test)
    print('***KNeighborsClassifier***')
    print('Confusion matrix')
    print(confusion_matrix(y_test,ypred))
    print('Classification report')
    print(classification_report(y_test,ypred))

def xgboost(x_train, x_test, y_train, y_test):
  xg = GradientBoostingClassifier()
  xg.fit(x_train,y_train)
  ypred = xg.predict(x_test)
  print('***GradientBoostingClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test,ypred))
  print('Classification report')
  print(classification_report(y_test,ypred))

# Importing the keras libraries and packages
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units=100, activation='relu',input_dim=11))

classifier.add(Dense(units=50, activation='relu'))

classifier.add(Dense(units=1, activation='sigmoid'))

classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model_history = classifier.fit(x_train, y_train, batch_size=100, validation_spit=0.2, epochs=100)

y_pred = classifier.predict(X_test)

[237] y_pred

[238] y_pred = (y_pred > 0.5)
      y_pred

[244] def predict_exit(sample_value):

sample_value = np.array(sample_value)

sample_value = sample_value.reshape(1, -1)

# Predictions
# Value Order 'CreditScore','Age','Tenure','Balence','NumoOfProducts','HasCrCard','IsActiveMember','EstimatedSalary','France','Germany','Spain','Female','Male'.
sample_value = [[1,1, 0, 1, 1, 4276, 1542,145, 240, 1,1]]
if predict_exit(sample_value)>0.5:
     print('Prediction: High chance of Loan Approval!')
else:
     print('Prediction: Low chance of Loan Approval.')

def compareModel(x_train,X_test,y_train,y_test):
  decisionTree(x_train,x_test,y_train,y_test)
  print('-'*100)
  RandomForest(x_train,x_test,y_train,y_test)
  print('-'*100)
  XGB(x_train,x_test,y_train,y_test)
  print('-'*100)
  KNN(x_train,x_test,y_train,y_test)
  print('-'*100)

compareModel(x_train,x_test,y_train,y_test)

yPred = classifier.predict(x_test)
print(accuracy_score(y_pred,y_test))
print("ANN Model")
print("Confution_Matrix")
print(confution_matrix(y_test,y_pred))
print("Classification Report")
print(classification_report(y_test,y_pred))